{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas emoji openai tiktoken sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the requisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# Loading data from CSV files\n",
    "test_data = pd.read_csv(\"./data/issues/issues_test.csv\")\n",
    "train_data = pd.read_csv(\"./data/issues/issues_train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>created_at</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-08-02 02:26:00</td>\n",
       "      <td>bug</td>\n",
       "      <td>Bug: [18.3.0-canary] renderToString hoists som...</td>\n",
       "      <td>&lt;!--\\r\\n  Please provide a clear and concise d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-07-17 22:43:05</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug]: Chrome extension gets disconne...</td>\n",
       "      <td>### Website or app\\r\\n\\r\\nhttps://react.dev/\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-07-13 19:01:47</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug]: Deprecated __REACT_DEVTOOLS_GL...</td>\n",
       "      <td>### Website or app\\r\\n\\r\\nN/A\\r\\n\\r\\n### Repro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-06-07 17:26:43</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug]: React devtools stuck at Loadin...</td>\n",
       "      <td>### Website or app\\r\\n\\r\\ncorporate project (p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-05-31 15:17:41</td>\n",
       "      <td>bug</td>\n",
       "      <td>Bug: Radio button onChange not called in curre...</td>\n",
       "      <td>&lt;!--\\r\\n  Please provide a clear and concise d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-22 11:52:21</td>\n",
       "      <td>feature</td>\n",
       "      <td>Task: GCC 12 support</td>\n",
       "      <td>Support compilation with GCC 12 and fix tests\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-16 19:27:55</td>\n",
       "      <td>feature</td>\n",
       "      <td>AudioIO: add dnn speech recognition sample on C++</td>\n",
       "      <td>### Pull Request Readiness Checklist\\r\\n\\r\\nSe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-14 22:05:58</td>\n",
       "      <td>feature</td>\n",
       "      <td>Use modern OpenVINO package interface</td>\n",
       "      <td>* new cmake options: `WITH_OPENVINO`, `OPENCV_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-12 09:14:41</td>\n",
       "      <td>feature</td>\n",
       "      <td>TiffEncoder write support more depth type</td>\n",
       "      <td>**Merge with extra**: https://github.com/openc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-06 11:01:32</td>\n",
       "      <td>feature</td>\n",
       "      <td>tiff need check TIFFTAG_SAMPLEFORMAT, should n...</td>\n",
       "      <td>### Pull Request Readiness Checklist\\r\\n\\r\\nSe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                repo           created_at    label  \\\n",
       "0     facebook/react  2023-08-02 02:26:00      bug   \n",
       "1     facebook/react  2023-07-17 22:43:05      bug   \n",
       "2     facebook/react  2023-07-13 19:01:47      bug   \n",
       "3     facebook/react  2023-06-07 17:26:43      bug   \n",
       "4     facebook/react  2023-05-31 15:17:41      bug   \n",
       "...              ...                  ...      ...   \n",
       "1495   opencv/opencv  2022-01-22 11:52:21  feature   \n",
       "1496   opencv/opencv  2022-01-16 19:27:55  feature   \n",
       "1497   opencv/opencv  2022-01-14 22:05:58  feature   \n",
       "1498   opencv/opencv  2022-01-12 09:14:41  feature   \n",
       "1499   opencv/opencv  2022-01-06 11:01:32  feature   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Bug: [18.3.0-canary] renderToString hoists som...   \n",
       "1     [DevTools Bug]: Chrome extension gets disconne...   \n",
       "2     [DevTools Bug]: Deprecated __REACT_DEVTOOLS_GL...   \n",
       "3     [DevTools Bug]: React devtools stuck at Loadin...   \n",
       "4     Bug: Radio button onChange not called in curre...   \n",
       "...                                                 ...   \n",
       "1495                               Task: GCC 12 support   \n",
       "1496  AudioIO: add dnn speech recognition sample on C++   \n",
       "1497              Use modern OpenVINO package interface   \n",
       "1498          TiffEncoder write support more depth type   \n",
       "1499  tiff need check TIFFTAG_SAMPLEFORMAT, should n...   \n",
       "\n",
       "                                                   body  \n",
       "0     <!--\\r\\n  Please provide a clear and concise d...  \n",
       "1     ### Website or app\\r\\n\\r\\nhttps://react.dev/\\r...  \n",
       "2     ### Website or app\\r\\n\\r\\nN/A\\r\\n\\r\\n### Repro...  \n",
       "3     ### Website or app\\r\\n\\r\\ncorporate project (p...  \n",
       "4     <!--\\r\\n  Please provide a clear and concise d...  \n",
       "...                                                 ...  \n",
       "1495  Support compilation with GCC 12 and fix tests\\...  \n",
       "1496  ### Pull Request Readiness Checklist\\r\\n\\r\\nSe...  \n",
       "1497  * new cmake options: `WITH_OPENVINO`, `OPENCV_...  \n",
       "1498  **Merge with extra**: https://github.com/openc...  \n",
       "1499  ### Pull Request Readiness Checklist\\r\\n\\r\\nSe...  \n",
       "\n",
       "[1500 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>created_at</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-08-26 06:33:37</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug] Cannot add node \"1\" because a n...</td>\n",
       "      <td>### Website or app\\n\\nPrivate repo cannot give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-07-28 05:16:12</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug]: Devtools extension build faili...</td>\n",
       "      <td>### Website or app\\n\\nN/A\\n\\n### Repro steps\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-07-13 21:58:31</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug]: Deprecated __REACT_DEVTOOLS_GL...</td>\n",
       "      <td>### Website or app\\n\\nhttps://github.com/open-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-06-14 02:31:20</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug] Cannot remove node \"0\" because ...</td>\n",
       "      <td>### Website or app\\n\\nlocal\\n\\n### Repro steps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>2023-06-03 11:29:44</td>\n",
       "      <td>bug</td>\n",
       "      <td>[DevTools Bug] Cannot remove node \"103\" becaus...</td>\n",
       "      <td>### Website or app\\n\\nlocalhost\\n\\n### Repro s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-24 10:48:13</td>\n",
       "      <td>feature</td>\n",
       "      <td>core: FP denormals support</td>\n",
       "      <td>relates #21046\\r\\n\\r\\n- support x86 SSE FTZ+DA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-20 12:40:55</td>\n",
       "      <td>feature</td>\n",
       "      <td>feature: submodule or a class scope for export...</td>\n",
       "      <td>All classes are registered in the scope that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-15 02:39:22</td>\n",
       "      <td>feature</td>\n",
       "      <td>Reading BigTiff images</td>\n",
       "      <td>**Merge with extra: https://github.com/opencv/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-14 15:37:53</td>\n",
       "      <td>feature</td>\n",
       "      <td>Add general broadcasting layer</td>\n",
       "      <td>Performance details(broadcasting 1x1 to 16x204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>2022-01-11 16:30:53</td>\n",
       "      <td>feature</td>\n",
       "      <td>Adapt remote inference to operate with NV12 blobs</td>\n",
       "      <td>### Pull Request Readiness Checklist\\r\\n\\r\\nSe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                repo           created_at    label  \\\n",
       "0     facebook/react  2023-08-26 06:33:37      bug   \n",
       "1     facebook/react  2023-07-28 05:16:12      bug   \n",
       "2     facebook/react  2023-07-13 21:58:31      bug   \n",
       "3     facebook/react  2023-06-14 02:31:20      bug   \n",
       "4     facebook/react  2023-06-03 11:29:44      bug   \n",
       "...              ...                  ...      ...   \n",
       "1495   opencv/opencv  2022-01-24 10:48:13  feature   \n",
       "1496   opencv/opencv  2022-01-20 12:40:55  feature   \n",
       "1497   opencv/opencv  2022-01-15 02:39:22  feature   \n",
       "1498   opencv/opencv  2022-01-14 15:37:53  feature   \n",
       "1499   opencv/opencv  2022-01-11 16:30:53  feature   \n",
       "\n",
       "                                                  title  \\\n",
       "0     [DevTools Bug] Cannot add node \"1\" because a n...   \n",
       "1     [DevTools Bug]: Devtools extension build faili...   \n",
       "2     [DevTools Bug]: Deprecated __REACT_DEVTOOLS_GL...   \n",
       "3     [DevTools Bug] Cannot remove node \"0\" because ...   \n",
       "4     [DevTools Bug] Cannot remove node \"103\" becaus...   \n",
       "...                                                 ...   \n",
       "1495                         core: FP denormals support   \n",
       "1496  feature: submodule or a class scope for export...   \n",
       "1497                             Reading BigTiff images   \n",
       "1498                     Add general broadcasting layer   \n",
       "1499  Adapt remote inference to operate with NV12 blobs   \n",
       "\n",
       "                                                   body  \n",
       "0     ### Website or app\\n\\nPrivate repo cannot give...  \n",
       "1     ### Website or app\\n\\nN/A\\n\\n### Repro steps\\n...  \n",
       "2     ### Website or app\\n\\nhttps://github.com/open-...  \n",
       "3     ### Website or app\\n\\nlocal\\n\\n### Repro steps...  \n",
       "4     ### Website or app\\n\\nlocalhost\\n\\n### Repro s...  \n",
       "...                                                 ...  \n",
       "1495  relates #21046\\r\\n\\r\\n- support x86 SSE FTZ+DA...  \n",
       "1496  All classes are registered in the scope that c...  \n",
       "1497  **Merge with extra: https://github.com/opencv/...  \n",
       "1498  Performance details(broadcasting 1x1 to 16x204...  \n",
       "1499  ### Pull Request Readiness Checklist\\r\\n\\r\\nSe...  \n",
       "\n",
       "[1500 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Data Cleaning: Method 1  \n",
    "Within this notebook, we employ two distinct data cleaning methodologies. This tailored approach is followed given variations among the repositories, each showing a more favorable outcome in response to one or the other cleaning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 2998 times.\n",
      "Returned original text 2 times.\n",
      "Cleaned 5998 times.\n",
      "Returned original text 2 times.\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for text cleaning\n",
    "cleaned_count = 0\n",
    "original_count = 0\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    global cleaned_count, original_count\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        original_count += 1\n",
    "        return text\n",
    "\n",
    "    # Remove double quotation marks\n",
    "    text = text.replace('\"', '')\n",
    "\n",
    "    # Remove text starting with \"DevTools\" and ending with \"(automated)\"\n",
    "    text = re.sub(r'DevTools.*?\\(automated\\)', '', text)\n",
    "\n",
    "    # Lowercasing should be one of the first steps to ensure uniformity\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove emojis\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n",
    "\n",
    "    # Remove '#' characters\n",
    "    text = text.replace(\"#\", \"\")\n",
    "\n",
    "    # Remove consecutive whitespaces and replace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove words that are over 20 characters\n",
    "    words = [word for word in words if len(word) <= 20]\n",
    "\n",
    "    # Join the remaining words back into cleaned text\n",
    "    cleaned_text = ' '.join(words)\n",
    "\n",
    "    cleaned_count += 1\n",
    "    return cleaned_text\n",
    "\n",
    "test_data['body'] = test_data['body'].apply(clean_text)\n",
    "test_data['title'] = test_data['title'].apply(clean_text)\n",
    "\n",
    "\n",
    "print(f\"Cleaned {cleaned_count} times.\")\n",
    "print(f\"Returned original text {original_count} times.\")\n",
    "\n",
    "train_data['body'] = train_data['body'].apply(clean_text)\n",
    "train_data['title'] = train_data['title'].apply(clean_text)\n",
    "\n",
    "\n",
    "print(f\"Cleaned {cleaned_count} times.\")\n",
    "print(f\"Returned original text {original_count} times.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Division  \n",
    "\n",
    "Subsequently, we partitioned our dataset into five smaller dataframes, ensuring an exclusive handling of each project. This segregation was executed on both the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_facebook = test_data[: 300]\n",
    "test_data_tensorflow = test_data[300: 600]\n",
    "test_data_microsoft = test_data[600: 900]\n",
    "test_data_bitcoin = test_data[900: 1200]\n",
    "test_data_opencv= test_data[1200: 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_facebook = train_data[: 300]\n",
    "train_data_tensorflow = train_data[300: 600]\n",
    "train_data_microsoft = train_data[600: 900]\n",
    "train_data_bitcoin = train_data[900: 1200]\n",
    "train_data_opencv= train_data[1200: 1500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning  \n",
    "We fine-tuned ChatGPT-3.5-Turbo using the training data, aiming to achieve superior performance compared to the standard approach of invoking the OpenAI API GPT-4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoking the API\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = 'open-ai-api')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook repository dataset fine-tuning process\n",
    "### Data Transformation  \n",
    "Prior to beginning the fine-tuning process, our initial step involves transforming our dataframe into a JSON line file format. This formatted file will serve as the prompt input for the fine-tuning process. Each prompt will encapsulate the title and body details of every pull request. Our anticipated outcome from the fine-tuned model will be the corresponding label for each PR, distinguishing between bug reports, questions, or feature requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('data/conversationaldata/conversational_data_facebook.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in train_data_facebook.iterrows():\n",
    "        # Create the user message by formatting the prompt with the title and body\n",
    "        user_message = f\"Classify, IN ONLY 1 WORD, the following GitHub issue as 'feature', 'bug', or 'question' based on its title and body:\\n{row['title']}\\n{row['body']}\"\n",
    "        \n",
    "        # Create the assistant message by taking the label\n",
    "        assistant_message = row['label']\n",
    "        \n",
    "        # Construct the conversation object\n",
    "        conversation_object = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"GitHub Issue Report Classifier\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write the conversation object to one line in the file\n",
    "        f.write(json.dumps(conversation_object, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training file  \n",
    "With our JSON line file generated, it now serves as the foundational conversation input for our fine-tuned model. We're prepared to upload this training file to the OpenAI API to initiate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-0Aub147vzoJf7tQGxAYXo2eF', bytes=363360, created_at=1701751545, filename='conversational_data_facebook.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Uplopading a training file\n",
    "ft_file_fb = client.files.create(\n",
    "  file=open(\"data/conversationaldata/conversational_data_facebook.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft_file_fb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation  \n",
    "At last, the stage is set to create the model, designated with the suffix 'repo-prissueclassifier.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-LG8EargXmVTFrGEWBYmF841X', created_at=1701751546, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-CpaRU3Zq9ePCCtbhezmcbgrg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-Mz8izUzswJ5M3H1C8S2aO6X0', validation_file=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a fine-tuned model\n",
    "ft_job_fb = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file_fb.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix= \"fb-issueclassifier\"\n",
    ")\n",
    "\n",
    "ft_job_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:gcucst440:fb-issueclassifier:8LLGMnAI\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the state of a fine-tune\n",
    "facebook_ft_model = client.fine_tuning.jobs.retrieve(ft_job_fb.id).fine_tuned_model\n",
    "print(facebook_ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can track the progress of your fine-tuning job by listing the lastest events. On our models it took about 3 hours to fine-tune each model\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_fb.id, limit=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow repository dataset fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('data/conversationaldata/conversational_data_tensorflow.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in train_data_tensorflow.iterrows():\n",
    "        # Create the user message by formatting the prompt with the title and body\n",
    "        user_message = f\"Classify, IN ONLY 1 WORD, the following GitHub issue as 'feature', 'bug', or 'question' based on its title and body:\\n{row['title']}\\n{row['body']}\"\n",
    "        \n",
    "        # Create the assistant message by taking the label\n",
    "        assistant_message = row['label']\n",
    "        \n",
    "        # Construct the conversation object\n",
    "        conversation_object = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"GitHub Issue Report Classifier\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write the conversation object to one line in the file\n",
    "        f.write(json.dumps(conversation_object, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-otqI5x1eZvI9ifBIdhE1buie', bytes=551677, created_at=1701751548, filename='conversational_data_tensorflow.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_file_tf = client.files.create(\n",
    "  file=open(\"data/conversationaldata/conversational_data_tensorflow.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft_file_tf.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-JPF1YSBlCiLAdOHc3egX9XjI', created_at=1701751548, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-CpaRU3Zq9ePCCtbhezmcbgrg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-poTXDVFaDaPWgI4OWkzKtHCY', validation_file=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a fine-tuned model\n",
    "ft_job_tf = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file_tf.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix= \"tf-issueclassifier\"\n",
    ")\n",
    "ft_job_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:gcucst440:tf-issueclassifier:8LLGZuRu\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the state of a fine-tune\n",
    "tensorflow_ft_model = client.fine_tuning.jobs.retrieve(ft_job_tf.id).fine_tuned_model\n",
    "print(tensorflow_ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can track the progress of your fine-tuning job by listing the lastest events. On our models it took about 3 hours to fine-tune each model\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_tf.id, limit=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft repository dataset fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('data/conversationaldata/conversational_data_microsoft.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in train_data_microsoft.iterrows():\n",
    "        # Create the user message by formatting the prompt with the title and body\n",
    "        user_message = f\"Classify, IN ONLY 1 WORD, the following GitHub issue as 'feature', 'bug', or 'question' based on its title and body:\\n{row['title']}\\n{row['body']}\"\n",
    "        \n",
    "        # Create the assistant message by taking the label\n",
    "        assistant_message = row['label']\n",
    "        \n",
    "        # Construct the conversation object\n",
    "        conversation_object = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"GitHub Issue Report Classifier\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write the conversation object to one line in the file\n",
    "        f.write(json.dumps(conversation_object, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-SPe4gJF5xHVBw1bfley7BddS', bytes=300117, created_at=1701751551, filename='conversational_data_microsoft.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_file_ms = client.files.create(\n",
    "  file=open(\"data/conversationaldata/conversational_data_microsoft.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft_file_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-fDSapt9Eh3trHScN7SOW5wIn', created_at=1701829792, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-CpaRU3Zq9ePCCtbhezmcbgrg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-o2pr8FckpXuZhncbVF9Fykzl', validation_file=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a fine-tuned model\n",
    "ft_job_ms = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file_ms.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix= \"ms-issueclassifier\"\n",
    ")\n",
    "ft_job_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:gcucst440:ms-issueclassifier:8LLFl5QI\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the state of a fine-tune\n",
    "microsoft_ft_model = client.fine_tuning.jobs.retrieve(ft_job_ms.id).fine_tuned_model\n",
    "print(microsoft_ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can track the progress of your fine-tuning job by listing the lastest events. On our models it took about 3 hours to fine-tune each model\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_ms.id, limit=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitcoin repository dataset fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('data/conversationaldata/conversational_data_bitcoin.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in train_data_bitcoin.iterrows():\n",
    "        # Create the user message by formatting the prompt with the title and body\n",
    "        user_message = f\"Classify, IN ONLY 1 WORD, the following GitHub issue as 'feature', 'bug', or 'question' based on its title and body:\\n{row['title']}\\n{row['body']}\"\n",
    "        \n",
    "        # Create the assistant message by taking the label\n",
    "        assistant_message = row['label']\n",
    "        \n",
    "        # Construct the conversation object\n",
    "        conversation_object = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"GitHub Issue Report Classifier\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write the conversation object to one line in the file\n",
    "        f.write(json.dumps(conversation_object, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-jDVrdHpKeEy23ttxa5KCfk7O', bytes=548310, created_at=1700103758, filename='conversational_data_bitcoin.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_file_bc = client.files.create(\n",
    "  file=open(\"data/conversationaldata/conversational_data_bitcoin.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft_file_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-dBpnVHyaN48iF0SC5QMwzCCn', created_at=1700103775, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-CpaRU3Zq9ePCCtbhezmcbgrg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-jDVrdHpKeEy23ttxa5KCfk7O', validation_file=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a fine-tuned model\n",
    "ft_job_bc = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file_bc.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix= \"bc-issueclassifier\"\n",
    ")\n",
    "ft_job_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:gcucst440:bc-issueclassifier:8LOOptG5\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the state of a fine-tune\n",
    "bitcoin_ft_model = client.fine_tuning.jobs.retrieve(ft_job_bc.id).fine_tuned_model\n",
    "print(bitcoin_ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can track the progress of your fine-tuning job by listing the lastest events. On our models it took about 3 hours to fine-tune each model\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_bc.id, limit=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV repository dataset fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('data/conversationaldata/conversational_data_opencv.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in train_data_opencv.iterrows():\n",
    "        # Create the user message by formatting the prompt with the title and body\n",
    "        user_message = f\"Classify, IN ONLY 1 WORD, the following GitHub issue as 'feature', 'bug', or 'question' based on its title and body:\\n{row['title']}\\n{row['body']}\"\n",
    "        \n",
    "        # Create the assistant message by taking the label\n",
    "        assistant_message = row['label']\n",
    "        \n",
    "        # Construct the conversation object\n",
    "        conversation_object = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"GitHub Issue Report Classifier\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write the conversation object to one line in the file\n",
    "        f.write(json.dumps(conversation_object, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-31FW3RwGmDhMZnIEtc95Oj5t', bytes=621942, created_at=1700104001, filename='conversational_data_opencv.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_file_oc = client.files.create(\n",
    "  file=open(\"data/conversationaldata/conversational_data_opencv.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft_file_oc.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-XdaiNJQtq10dbFhMPdDfKk4t', created_at=1700104041, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-CpaRU3Zq9ePCCtbhezmcbgrg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-31FW3RwGmDhMZnIEtc95Oj5t', validation_file=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a fine-tuned model\n",
    "ft_job_oc = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file_oc.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix= \"cv-issueclassifier\"\n",
    ")\n",
    "ft_job_oc.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:gcucst440:cv-issueclassifier:8LOdV6zV\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the state of a fine-tune\n",
    "opencv_ft_model = client.fine_tuning.jobs.retrieve(ft_job_oc.id).fine_tuned_model\n",
    "print(opencv_ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can track the progress of your fine-tuning job by listing the lastest events. On our models it took about 3 hours to fine-tune each model\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_oc.id, limit=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning results  \n",
    "The successful fine-tuning of all models was completed using the default of 3 epochs. The process spanned approximately 5 hours; however, variations in processing time might occur due to queue dynamics at any given moment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Fine-tuned model  \n",
    "Next, another API from OpenAI is used to invoke the fine-tuned model and assess its performance on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import concurrent.futures\n",
    "import tiktoken\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Replace 'open-ai-key' with your actual OpenAI API key\n",
    "openai.api_key = 'open-ai-api'\n",
    "\n",
    "# max_token here should be one since 'bug', 'feature', and 'question' are one token long. This might change for future versions of the model and api but you can check the value on the\n",
    "def query_chatgpt(prompt, model, temperature=0.0,  max_tokens=1, max_retries=5):\n",
    "    \"\"\"\n",
    "    Function to query ChatGPT-4 with a given prompt, with retries for timeouts.\n",
    "\n",
    "    :param prompt: Prompt string to send to ChatGPT-2.5\n",
    "    :param model: The model to use, default is ChatGPT-3.5\n",
    "    :param max_tokens: Maximum number of tokens to generate\n",
    "    :param max_retries: Maximum number of retries for timeout\n",
    "    :return: Response from ChatGPT-3.5 or None if all retries fail\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    max_content_tokens = 3999\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Function to truncate the message and avoid passing the limit of 4k tokens per gpt-3.5 fine-tuned model limitations\n",
    "    def truncate_message(message, max_length):\n",
    "        tokens = encoding.encode(message)\n",
    "        if len(tokens) > max_length:\n",
    "            truncated_tokens = tokens[:max_length]\n",
    "            message = encoding.decode(truncated_tokens)\n",
    "        return message\n",
    "\n",
    "    # Truncate the prompt if necessary\n",
    "    prompt = truncate_message(prompt, max_content_tokens)\n",
    "\n",
    "    while attempt < max_retries:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(\n",
    "                openai.chat.completions.create,\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"GitHub Issue Report Classifier\"}, {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            try:\n",
    "                response = future.result(timeout=5)  # 5 seconds timeout\n",
    "                return response.choices[0].message.content\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                print(f\"Attempt {attempt + 1}/{max_retries} - Request timed out. Retrying...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1}/{max_retries} - An error occurred: {e}\")\n",
    "            finally:\n",
    "                attempt += 1\n",
    "\n",
    "    print(\"Failed to get a response after several retries.\")\n",
    "    return None\n",
    "    \n",
    "labels = ['feature', 'bug', 'question']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook React Repo Testing  \n",
    "The function defined above is being called, passing the specific model for each repository and testing it with the testing dataset. It's essential to note the setup of a timer to comply with the \"token per minute\" limitations on the API. Additionally, the results of each iteration are printed for tracking and improvement purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_fb = []\n",
    "y_pred_fb = []\n",
    "\n",
    "iterations = len(test_data_facebook)\n",
    "\n",
    "# Now let's loop through the test data and classify the GitHub issues\n",
    "for i in range(iterations):\n",
    "    correct_label = test_data_facebook.iloc[i]['label'].lower()\n",
    "    description = f\"{test_data_facebook.iloc[i]['title']} \\n {test_data_facebook.iloc[i]['body']}\"\n",
    "    print(f\"Correct PR type: {correct_label}\")\n",
    "    \n",
    "    prompt = f\"Classify, IN ONLY 1 WORD, the following GitHub issue as 'feature', 'bug', or 'question' based on its title and body:\\n{description}\"\n",
    "    response = query_chatgpt(prompt, facebook_ft_model)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Failed to get a response after several retries. Skipping this item.\")\n",
    "        continue  # Skip this iteration and move to the next one\n",
    "    \n",
    "    # Clean the response to keep only letters (and optionally numbers)\n",
    "    predicted_label = re.sub(r'[^A-Za-z]+', '', response).lower().strip()\n",
    "    print(f\"Predicted PR type: {predicted_label}\")\n",
    "    \n",
    "    # Append to lists for evaluation\n",
    "    y_true_fb.append(correct_label)\n",
    "    y_pred_fb.append(predicted_label)\n",
    "    time.sleep(6)  # Wait for 6 seconds before retrying\n",
    "\n",
    "# See output on outputs/cell51output.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the results  \n",
    "Once all testing data undergoes evaluation using the corresponding fine-tuned models, we'll leverage the two arrays generatedâ€”representing the true labels and predicted labelsâ€”to conduct result assessments.\n",
    "\n",
    "For tracking purposes, a CSV file has been generated for each result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: {'TP': 89, 'FP': 15, 'FN': 11, 'TN': 185}\n",
      "bug: {'TP': 95, 'FP': 19, 'FN': 5, 'TN': 181}\n",
      "question: {'TP': 74, 'FP': 8, 'FN': 26, 'TN': 192}\n",
      "Precision = 0.8638471961642693\n",
      "Recall = 0.86\n",
      "F1-score = 0.8578621000281252\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average F1-score, precision, and recall\n",
    "f1_fb = f1_score(y_true_fb, y_pred_fb, labels=labels, average='weighted')\n",
    "precision_fb = precision_score(y_true_fb, y_pred_fb, labels=labels, average='weighted')\n",
    "recall_fb = recall_score(y_true_fb, y_pred_fb, labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm_fb = confusion_matrix(y_true_fb, y_pred_fb, labels=labels)\n",
    "\n",
    "cm_df_fb = pd.DataFrame(cm_fb, index=labels, columns=labels)\n",
    "\n",
    "# Calculate TP, FP, FN, TN\n",
    "results_fb = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results_fb[label] = {'TP': cm_fb[i, i]}\n",
    "    results_fb[label]['FP'] = cm_fb[:, i].sum() - cm_fb[i, i]\n",
    "    results_fb[label]['FN'] = cm_fb[i, :].sum() - cm_fb[i, i]\n",
    "    results_fb[label]['TN'] = cm_fb.sum() - (results_fb[label]['TP'] + results_fb[label]['FP'] + results_fb[label]['FN'])\n",
    "\n",
    "# Print results_fb\n",
    "for label, metrics in results_fb.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results_fb to CSV\n",
    "results_fb_df = pd.DataFrame(results_fb).T\n",
    "results_fb_df['F1-score'] = f1_fb\n",
    "results_fb_df['Recall'] = recall_fb\n",
    "results_fb_df['Precision'] = precision_fb\n",
    "\n",
    "results_fb_df.to_csv('metrics/confusion_matrix_fb.csv')\n",
    "\n",
    "print(f\"Precision = {precision_fb}\")\n",
    "print(f\"Recall = {recall_fb}\")\n",
    "print(f\"F1-score = {f1_fb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score: 85.79%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the metrics  \n",
    "Below, the metrics for each label are presented to facilitate a more precise evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  recall  f1-score  support\n",
      "bug            0.833333    0.95  0.887850   100.00\n",
      "feature        0.855769    0.89  0.872549   100.00\n",
      "question       0.902439    0.74  0.813187   100.00\n",
      "accuracy       0.860000    0.86  0.860000     0.86\n",
      "macro avg      0.863847    0.86  0.857862   300.00\n",
      "weighted avg   0.863847    0.86  0.857862   300.00\n"
     ]
    }
   ],
   "source": [
    "# Create a classification report\n",
    "report_fb = classification_report(y_true_fb, y_pred_fb, labels=['bug', 'feature', 'question'], target_names=['bug', 'feature', 'question'], zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df_fb = pd.DataFrame(report_fb).transpose()\n",
    "\n",
    "# Print the classification report\n",
    "print(report_df_fb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Repo Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_tf = []\n",
    "y_pred_tf = []\n",
    "\n",
    "iterations = len(test_data_tensorflow)\n",
    "\n",
    "# Now let's loop through the test data and classify the GitHub pull requests\n",
    "for i in range(iterations):\n",
    "    correct_label = test_data_tensorflow.iloc[i]['label'].lower()\n",
    "    description = f\"{test_data_tensorflow.iloc[i]['title']} \\n {test_data_tensorflow.iloc[i]['body']}\"\n",
    "    print(f\"Correct PR type: {correct_label}\")\n",
    "    \n",
    "    prompt = f\"Classify, IN ONLY 1 WORD, the following GitHub pull request as 'feature', 'bug', or 'question' based on its title and body:\\n{description}\"\n",
    "    response = query_chatgpt(prompt, tensorflow_ft_model)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Failed to get a response after several retries. Skipping this item.\")\n",
    "        continue  # Skip this iteration and move to the next one\n",
    "    \n",
    "    # Clean the response to keep only letters (and optionally numbers)\n",
    "    predicted_label = re.sub(r'[^A-Za-z]+', '', response).lower().strip()\n",
    "    print(f\"Predicted PR type: {predicted_label}\")\n",
    "    \n",
    "    # Append to lists for evaluation\n",
    "    y_true_tf.append(correct_label)\n",
    "    y_pred_tf.append(predicted_label)\n",
    "    time.sleep(6)  # Wait for 6 seconds before retrying\n",
    "\n",
    "# See output on outputs/cell58output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: {'TP': 81, 'FP': 6, 'FN': 19, 'TN': 194}\n",
      "bug: {'TP': 88, 'FP': 8, 'FN': 12, 'TN': 192}\n",
      "question: {'TP': 89, 'FP': 28, 'FN': 11, 'TN': 172}\n",
      "Precision = 0.869461636703016\n",
      "Recall = 0.86\n",
      "F1-score = 0.861515280599043\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average F1-score, precision, and recall\n",
    "f1_tf = f1_score(y_true_tf, y_pred_tf, labels=labels, average='weighted')\n",
    "precision_tf = precision_score(y_true_tf, y_pred_tf, labels=labels, average='weighted')\n",
    "recall_tf = recall_score(y_true_tf, y_pred_tf, labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm_tf = confusion_matrix(y_true_tf, y_pred_tf, labels=labels)\n",
    "\n",
    "cm_df_tf = pd.DataFrame(cm_tf, index=labels, columns=labels)\n",
    "\n",
    "# Calculate TP, FP, FN, TN\n",
    "results_tf = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results_tf[label] = {'TP': cm_tf[i, i]}\n",
    "    results_tf[label]['FP'] = cm_tf[:, i].sum() - cm_tf[i, i]\n",
    "    results_tf[label]['FN'] = cm_tf[i, :].sum() - cm_tf[i, i]\n",
    "    results_tf[label]['TN'] = cm_tf.sum() - (results_tf[label]['TP'] + results_tf[label]['FP'] + results_tf[label]['FN'])\n",
    "\n",
    "# Print results_tf\n",
    "for label, metrics in results_tf.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results_tf to CSV\n",
    "results_tf_df = pd.DataFrame(results_tf).T\n",
    "results_tf_df['F1-score'] = f1_tf\n",
    "results_tf_df['Recall'] = recall_tf\n",
    "results_tf_df['Precision'] = precision_tf\n",
    "\n",
    "results_tf_df.to_csv('metrics/confusion_matrix_tf.csv')\n",
    "\n",
    "print(f\"Precision = {precision_tf}\")\n",
    "print(f\"Recall = {recall_tf}\")\n",
    "print(f\"F1-score = {f1_tf}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score: 86.15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  recall  f1-score  support\n",
      "bug            0.916667    0.88  0.897959   100.00\n",
      "feature        0.931034    0.81  0.866310   100.00\n",
      "question       0.760684    0.89  0.820276   100.00\n",
      "accuracy       0.860000    0.86  0.860000     0.86\n",
      "macro avg      0.869462    0.86  0.861515   300.00\n",
      "weighted avg   0.869462    0.86  0.861515   300.00\n"
     ]
    }
   ],
   "source": [
    "# Create a classification report\n",
    "report_tf = classification_report(y_true_tf, y_pred_tf, labels=['bug', 'feature', 'question'], target_names=['bug', 'feature', 'question'], zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df_tf = pd.DataFrame(report_tf).transpose()\n",
    "\n",
    "# Print the classification report\n",
    "print(report_df_tf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft Repo Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_ms = []\n",
    "y_pred_ms = []\n",
    "\n",
    "iterations = len(test_data_microsoft)\n",
    "\n",
    "# Now let's loop through the test data and classify the GitHub pull requests\n",
    "for i in range(iterations):\n",
    "    correct_label = test_data_microsoft.iloc[i]['label'].lower()\n",
    "    description = f\"{test_data_microsoft.iloc[i]['title']}\\n{test_data_microsoft.iloc[i]['body']}\"\n",
    "    print(f\"Correct PR type: {correct_label}\")\n",
    "    \n",
    "    prompt = f\"Classify, IN ONLY 1 WORD, the following GitHub pull request as 'feature', 'bug', or 'question' based on its title and body:\\n{description}\"\n",
    "    response = query_chatgpt(prompt, microsoft_ft_model)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Failed to get a response after several retries. Skipping this item.\")\n",
    "        continue  # Skip this iteration and move to the next one\n",
    "    \n",
    "    # Clean the response to keep only letters (and optionally numbers)\n",
    "    predicted_label = re.sub(r'[^A-Za-z]+', '', response).lower().strip()\n",
    "    print(f\"Predicted PR type: {predicted_label}\")\n",
    "    \n",
    "    # Append to lists for evaluation\n",
    "    y_true_ms.append(correct_label)\n",
    "    y_pred_ms.append(predicted_label)\n",
    "    time.sleep(6)  # Wait for 6 seconds before retrying\n",
    "\n",
    "# See output on outputs/cell63output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: {'TP': 84, 'FP': 21, 'FN': 16, 'TN': 179}\n",
      "bug: {'TP': 76, 'FP': 13, 'FN': 24, 'TN': 187}\n",
      "question: {'TP': 80, 'FP': 26, 'FN': 20, 'TN': 174}\n",
      "Precision = 0.802883188467246\n",
      "Recall = 0.8\n",
      "F1-score = 0.8001480094936562\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average F1-score, precision, and recall\n",
    "f1_ms = f1_score(y_true_ms, y_pred_ms, labels=labels, average='weighted')\n",
    "precision_ms = precision_score(y_true_ms, y_pred_ms, labels=labels, average='weighted')\n",
    "recall_ms = recall_score(y_true_ms, y_pred_ms, labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm_ms = confusion_matrix(y_true_ms, y_pred_ms, labels=labels)\n",
    "\n",
    "cm_df_ms = pd.DataFrame(cm_ms, index=labels, columns=labels)\n",
    "\n",
    "# Calculate TP, FP, FN, TN\n",
    "results_ms = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results_ms[label] = {'TP': cm_ms[i, i]}\n",
    "    results_ms[label]['FP'] = cm_ms[:, i].sum() - cm_ms[i, i]\n",
    "    results_ms[label]['FN'] = cm_ms[i, :].sum() - cm_ms[i, i]\n",
    "    results_ms[label]['TN'] = cm_ms.sum() - (results_ms[label]['TP'] + results_ms[label]['FP'] + results_ms[label]['FN'])\n",
    "\n",
    "# Print results_ms\n",
    "for label, metrics in results_ms.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results_ms to CSV\n",
    "results_ms_df = pd.DataFrame(results_ms).T\n",
    "results_ms_df['F1-score'] = f1_ms\n",
    "results_ms_df['Recall'] = recall_ms\n",
    "results_ms_df['Precision'] = precision_ms\n",
    "\n",
    "\n",
    "results_ms_df.to_csv('metrics/confusion_matrix_ms.csv')\n",
    "\n",
    "print(f\"Precision = {precision_ms}\")\n",
    "print(f\"Recall = {recall_ms}\")\n",
    "print(f\"F1-score = {f1_ms}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score: 80.01%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  recall  f1-score  support\n",
      "bug            0.853933    0.76  0.804233    100.0\n",
      "feature        0.800000    0.84  0.819512    100.0\n",
      "question       0.754717    0.80  0.776699    100.0\n",
      "accuracy       0.800000    0.80  0.800000      0.8\n",
      "macro avg      0.802883    0.80  0.800148    300.0\n",
      "weighted avg   0.802883    0.80  0.800148    300.0\n"
     ]
    }
   ],
   "source": [
    "# Create a classification report\n",
    "report_ms = classification_report(y_true_ms, y_pred_ms, labels=['bug', 'feature', 'question'], target_names=['bug', 'feature', 'question'], zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df_ms = pd.DataFrame(report_ms).transpose()\n",
    "\n",
    "# Print the classification report\n",
    "print(report_df_ms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitcoin Repo Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_bc = []\n",
    "y_pred_bc = []\n",
    "\n",
    "iterations = len(test_data_bitcoin)\n",
    "\n",
    "# Now let's loop through the test data and classify the GitHub pull requests\n",
    "for i in range(iterations):\n",
    "    correct_label = test_data_bitcoin.iloc[i]['label'].lower()\n",
    "    description = f\"{test_data_bitcoin.iloc[i]['title']}\\n{test_data_bitcoin.iloc[i]['body']}\"\n",
    "    print(f\"Correct PR type: {correct_label}\")\n",
    "    \n",
    "    prompt = f\"Classify, IN ONLY 1 WORD, the following GitHub pull request as 'feature', 'bug', or 'question' based on its title and body:\\n{description}\"\n",
    "    response = query_chatgpt(prompt, bitcoin_ft_model)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Failed to get a response after several retries. Skipping this item.\")\n",
    "        continue  # Skip this iteration and move to the next one\n",
    "    \n",
    "    # Clean the response to keep only letters (and optionally numbers)\n",
    "    predicted_label = re.sub(r'[^A-Za-z]+', '', response).lower().strip()\n",
    "    print(f\"Predicted PR type: {predicted_label}\")\n",
    "    \n",
    "    # Append to lists for evaluation\n",
    "    y_true_bc.append(correct_label)\n",
    "    y_pred_bc.append(predicted_label)\n",
    "    time.sleep(6)  # Wait for 6 seconds before retrying\n",
    "\n",
    "# See output on outputs/cell68output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: {'TP': 89, 'FP': 18, 'FN': 11, 'TN': 182}\n",
      "bug: {'TP': 80, 'FP': 29, 'FN': 20, 'TN': 171}\n",
      "question: {'TP': 62, 'FP': 22, 'FN': 38, 'TN': 178}\n",
      "Precision = 0.7679386310527527\n",
      "Recall = 0.77\n",
      "F1-score = 0.7664555547850743\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average F1-score, precision, and recall\n",
    "f1_bc = f1_score(y_true_bc, y_pred_bc, labels=labels, average='weighted')\n",
    "precision_bc = precision_score(y_true_bc, y_pred_bc, labels=labels, average='weighted')\n",
    "recall_bc = recall_score(y_true_bc, y_pred_bc, labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm_bc = confusion_matrix(y_true_bc, y_pred_bc, labels=labels)\n",
    "\n",
    "cm_df_bc = pd.DataFrame(cm_bc, index=labels, columns=labels)\n",
    "\n",
    "# Calculate TP, FP, FN, TN\n",
    "results_bc = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results_bc[label] = {'TP': cm_bc[i, i]}\n",
    "    results_bc[label]['FP'] = cm_bc[:, i].sum() - cm_bc[i, i]\n",
    "    results_bc[label]['FN'] = cm_bc[i, :].sum() - cm_bc[i, i]\n",
    "    results_bc[label]['TN'] = cm_bc.sum() - (results_bc[label]['TP'] + results_bc[label]['FP'] + results_bc[label]['FN'])\n",
    "\n",
    "# Print results_bc\n",
    "for label, metrics in results_bc.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results_bc to CSV\n",
    "results_bc_df = pd.DataFrame(results_bc).T\n",
    "results_bc_df['F1-score'] = f1_bc\n",
    "results_bc_df['Recall'] = recall_bc\n",
    "results_bc_df['Precision'] = precision_bc\n",
    "\n",
    "results_bc_df.to_csv('metrics/confusion_matrix_bc.csv')\n",
    "\n",
    "print(f\"Precision = {precision_bc}\")\n",
    "print(f\"Recall = {recall_bc}\")\n",
    "print(f\"F1-score = {f1_bc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score: 76.65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  recall  f1-score  support\n",
      "bug            0.733945    0.80  0.765550   100.00\n",
      "feature        0.831776    0.89  0.859903   100.00\n",
      "question       0.738095    0.62  0.673913   100.00\n",
      "accuracy       0.770000    0.77  0.770000     0.77\n",
      "macro avg      0.767939    0.77  0.766456   300.00\n",
      "weighted avg   0.767939    0.77  0.766456   300.00\n"
     ]
    }
   ],
   "source": [
    "# Create a classification report\n",
    "report_bc = classification_report(y_true_bc, y_pred_bc, labels=['bug', 'feature', 'question'], target_names=['bug', 'feature', 'question'], zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df_bc = pd.DataFrame(report_bc).transpose()\n",
    "\n",
    "# Print the classification report\n",
    "print(report_df_bc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV Repo Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_oc = []\n",
    "y_pred_oc = []\n",
    "\n",
    "iterations = len(test_data_opencv)\n",
    "\n",
    "# Now let's loop through the test data and classify the GitHub pull requests\n",
    "for i in range(iterations):\n",
    "    correct_label = test_data_opencv.iloc[i]['label'].lower()\n",
    "    description = f\"{test_data_opencv.iloc[i]['title']}\\n{test_data_opencv.iloc[i]['body']}\"\n",
    "    print(f\"Correct PR type: {correct_label}\")\n",
    "    \n",
    "    prompt = f\"Classify, IN ONLY 1 WORD, the following GitHub pull request as 'feature', 'bug', or 'question' based on its title and body:\\n{description}\"\n",
    "    response = query_chatgpt(prompt, opencv_ft_model)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Failed to get a response after several retries. Skipping this item.\")\n",
    "        continue  # Skip this iteration and move to the next one\n",
    "    \n",
    "    # Clean the response to keep only letters (and optionally numbers)\n",
    "    predicted_label = re.sub(r'[^A-Za-z]+', '', response).lower().strip()\n",
    "    print(f\"Predicted PR type: {predicted_label}\")\n",
    "    \n",
    "    # Append to lists for evaluation\n",
    "    y_true_oc.append(correct_label)\n",
    "    y_pred_oc.append(predicted_label)\n",
    "    time.sleep(6)  # Wait for 6 seconds before retrying\n",
    "\n",
    "# See output on outputs/cell73output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: {'TP': 80, 'FP': 14, 'FN': 20, 'TN': 186}\n",
      "bug: {'TP': 82, 'FP': 28, 'FN': 18, 'TN': 172}\n",
      "question: {'TP': 81, 'FP': 15, 'FN': 19, 'TN': 185}\n",
      "Precision = 0.8134227917472598\n",
      "Recall = 0.81\n",
      "F1-score = 0.8107417537461722\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average F1-score, precision, and recall\n",
    "f1_oc = f1_score(y_true_oc, y_pred_oc, labels=labels, average='weighted')\n",
    "precision_oc = precision_score(y_true_oc, y_pred_oc, labels=labels, average='weighted')\n",
    "recall_oc = recall_score(y_true_oc, y_pred_oc, labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm_oc = confusion_matrix(y_true_oc, y_pred_oc, labels=labels)\n",
    "\n",
    "cm_df_oc = pd.DataFrame(cm_oc, index=labels, columns=labels)\n",
    "\n",
    "# Calculate TP, FP, FN, TN\n",
    "results_oc = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results_oc[label] = {'TP': cm_oc[i, i]}\n",
    "    results_oc[label]['FP'] = cm_oc[:, i].sum() - cm_oc[i, i]\n",
    "    results_oc[label]['FN'] = cm_oc[i, :].sum() - cm_oc[i, i]\n",
    "    results_oc[label]['TN'] = cm_oc.sum() - (results_oc[label]['TP'] + results_oc[label]['FP'] + results_oc[label]['FN'])\n",
    "\n",
    "# Print results_oc\n",
    "for label, metrics in results_oc.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results_oc to CSV\n",
    "results_oc_df = pd.DataFrame(results_oc).T\n",
    "results_oc_df['F1-score'] = f1_oc\n",
    "results_oc_df['Recall'] = recall_oc\n",
    "results_oc_df['Precision'] = precision_oc\n",
    "\n",
    "results_oc_df.to_csv('metrics/confusion_matrix_oc.csv')\n",
    "\n",
    "print(f\"Precision = {precision_oc}\")\n",
    "print(f\"Recall = {recall_oc}\")\n",
    "print(f\"F1-score = {f1_oc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score: 81.07%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  recall  f1-score  support\n",
      "bug            0.745455    0.82  0.780952   100.00\n",
      "feature        0.851064    0.80  0.824742   100.00\n",
      "question       0.843750    0.81  0.826531   100.00\n",
      "accuracy       0.810000    0.81  0.810000     0.81\n",
      "macro avg      0.813423    0.81  0.810742   300.00\n",
      "weighted avg   0.813423    0.81  0.810742   300.00\n"
     ]
    }
   ],
   "source": [
    "# Create a classification report\n",
    "report_oc = classification_report(y_true_oc, y_pred_oc, labels=['bug', 'feature', 'question'], target_names=['bug', 'feature', 'question'], zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df_oc = pd.DataFrame(report_oc).transpose()\n",
    "\n",
    "# Print the classification report\n",
    "print(report_df_oc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial results  \n",
    "With results gathered for all repositories tested against their respective trained models, we're poised to consolidate the confusion matrix data and derive the overall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Results: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Bug': {'Precision': 0.8166599999999999,\n",
       "  'Recall': 0.842,\n",
       "  'F1-Score': 0.8273199999999999},\n",
       " 'Feature': {'Precision': 0.8539200000000001,\n",
       "  'Recall': 0.8460000000000001,\n",
       "  'F1-Score': 0.8485800000000001},\n",
       " 'Question': {'Precision': 0.79994,\n",
       "  'Recall': 0.772,\n",
       "  'F1-Score': 0.7821200000000001},\n",
       " 'Average': {'Precision': 0.82344,\n",
       "  'Recall': 0.82,\n",
       "  'F1-Score': 0.8193399999999998}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_average(values):\n",
    "    total = 0\n",
    "    for value in values:\n",
    "        total += value\n",
    "    return total/len(values)\n",
    "\n",
    "### BUG ###\n",
    "facebook_bug_precision = 0.8333\n",
    "tensorflow_bug_precision = 0.9167 \n",
    "microsoft_bug_precision = 0.8539 \n",
    "bitcoin_bug_precision = 0.7339 \n",
    "opencv_bug_precision = 0.7455 \n",
    "bug_precision_values = [facebook_bug_precision, tensorflow_bug_precision, microsoft_bug_precision, bitcoin_bug_precision, opencv_bug_precision]\n",
    "\n",
    "facebook_bug_recall = 0.9500\n",
    "tensorflow_bug_recall = 0.88\n",
    "microsoft_bug_recall = 0.76\n",
    "bitcoin_bug_recall = 0.8000\n",
    "opencv_bug_recall = 0.8200 \n",
    "bug_recall_values = [facebook_bug_recall, tensorflow_bug_recall, microsoft_bug_recall, bitcoin_bug_recall, opencv_bug_recall]\n",
    "\n",
    "facebook_bug_f1 = 0.8878\n",
    "tensorflow_bug_f1 = 0.8980\n",
    "microsoft_bug_f1 = 0.8042\n",
    "bitcoin_bug_f1 = 0.7656\n",
    "opencv_bug_f1 = 0.7810\n",
    "bug_f1_values = [facebook_bug_f1, tensorflow_bug_f1, microsoft_bug_f1, bitcoin_bug_f1, opencv_bug_f1]\n",
    "\n",
    "\n",
    "### FEATURE ###\n",
    "facebook_feature_precision = 0.8557\n",
    "tensorflow_feature_precision = 0.9310\n",
    "microsoft_feature_precision = 0.80\n",
    "bitcoin_feature_precision = 0.8318\n",
    "opencv_feature_precision = 0.8511\n",
    "feature_precision_values = [facebook_feature_precision, tensorflow_feature_precision, microsoft_feature_precision, bitcoin_feature_precision, opencv_feature_precision]\n",
    "\n",
    "facebook_feature_recall = 0.8900\n",
    "tensorflow_feature_recall = 0.81\n",
    "microsoft_feature_recall = 0.84\n",
    "bitcoin_feature_recall = 0.8900\n",
    "opencv_feature_recall = 0.8000\n",
    "feature_recall_values = [facebook_feature_recall, tensorflow_feature_recall, microsoft_feature_recall, bitcoin_feature_recall, opencv_feature_recall]\n",
    "\n",
    "facebook_feature_f1 = 0.8725\n",
    "tensorflow_feature_f1 = 0.8663\n",
    "microsoft_feature_f1 = 0.8195\n",
    "bitcoin_feature_f1 = 0.8599\n",
    "opencv_feature_f1 = 0.8247\n",
    "feature_f1_values = [facebook_feature_f1, tensorflow_feature_f1, microsoft_feature_f1, bitcoin_feature_f1, opencv_feature_f1]\n",
    "\n",
    "\n",
    "### QUESTION ###\n",
    "facebook_question_precision = 0.9024\n",
    "tensorflow_question_precision = 0.7607\n",
    "microsoft_question_precision = 0.7547\n",
    "bitcoin_question_precision = 0.7381\n",
    "opencv_question_precision = 0.8438\n",
    "question_precision_values = [facebook_question_precision, tensorflow_question_precision, microsoft_question_precision, bitcoin_question_precision, opencv_question_precision]\n",
    "\n",
    "facebook_question_recall = 0.7400\n",
    "tensorflow_question_recall = 0.89\n",
    "microsoft_question_recall = 0.80\n",
    "bitcoin_question_recall = 0.6200\n",
    "opencv_question_recall = 0.8100\n",
    "question_recall_values = [facebook_question_recall, tensorflow_question_recall, microsoft_question_recall, bitcoin_question_recall, opencv_question_recall]\n",
    "\n",
    "facebook_question_f1 = 0.8132\n",
    "tensorflow_question_f1 = 0.8203\n",
    "microsoft_question_f1 = 0.7767\n",
    "bitcoin_question_f1 = 0.6739\n",
    "opencv_question_f1 = 0.8265\n",
    "question_f1_values = [facebook_question_f1, tensorflow_question_f1, microsoft_question_f1, bitcoin_question_f1, opencv_question_f1]\n",
    "\n",
    "\n",
    "### AVERAGE ###\n",
    "facebook_average_precision = 0.8635\n",
    "tensorflow_average_precision = 0.8695\n",
    "microsoft_average_precision = 0.8029\n",
    "bitcoin_average_precision = 0.7679\n",
    "opencv_average_precision = 0.8134\n",
    "average_precision_values = [facebook_average_precision, tensorflow_average_precision, microsoft_average_precision, bitcoin_average_precision, opencv_average_precision]\n",
    "\n",
    "facebook_average_recall = 0.8600\n",
    "tensorflow_average_recall = 0.86\n",
    "microsoft_average_recall = 0.8\n",
    "bitcoin_average_recall = 0.7700\n",
    "opencv_average_recall = 0.8100\n",
    "average_recall_values = [facebook_average_recall, tensorflow_average_recall, microsoft_average_recall, bitcoin_average_recall, opencv_average_recall]\n",
    "\n",
    "facebook_average_f1 = 0.8579\n",
    "tensorflow_average_f1 = 0.8615\n",
    "microsoft_average_f1 = 0.8001\n",
    "bitcoin_average_f1 = 0.7665\n",
    "opencv_average_f1 = 0.8107\n",
    "average_f1_values = [facebook_average_f1, tensorflow_average_f1, microsoft_average_f1, bitcoin_average_f1, opencv_average_f1]\n",
    "\n",
    "# Calculating overall (average) values  \n",
    "overall_bug_f1 = calculate_average(bug_f1_values)\n",
    "overall_bug_precision = calculate_average(bug_precision_values)\n",
    "overall_bug_recall = calculate_average(bug_recall_values)\n",
    "overall_feature_f1 = calculate_average(feature_f1_values)\n",
    "overall_feature_precision = calculate_average(feature_precision_values)\n",
    "overall_feature_recall = calculate_average(feature_recall_values)\n",
    "overall_question_f1 = calculate_average(question_f1_values)\n",
    "overall_question_precision = calculate_average(question_precision_values)\n",
    "overall_question_recall = calculate_average(question_recall_values)\n",
    "overall_average_f1 = calculate_average(average_f1_values)\n",
    "overall_average_precision = calculate_average(average_precision_values)\n",
    "overall_average_recall = calculate_average(average_recall_values)\n",
    "\n",
    "print(\"Overall Results: \")\n",
    "# Formatting the results\n",
    "formatted_metrics = {\n",
    "    \"Bug\": {\n",
    "        \"Precision\": overall_bug_precision, \n",
    "        \"Recall\": overall_bug_recall, \n",
    "        \"F1-Score\": overall_bug_f1\n",
    "    },\n",
    "    \"Feature\": {\n",
    "        \"Precision\": overall_feature_precision, \n",
    "        \"Recall\": overall_feature_recall, \n",
    "        \"F1-Score\": overall_feature_f1\n",
    "    },\n",
    "    \"Question\": {\n",
    "        \"Precision\": overall_question_precision, \n",
    "        \"Recall\": overall_question_recall, \n",
    "        \"F1-Score\": overall_question_f1\n",
    "    },\n",
    "    \"Average\": {\n",
    "        \"Precision\": overall_average_precision, \n",
    "        \"Recall\": overall_average_recall, \n",
    "        \"F1-Score\": overall_average_f1\n",
    "    }\n",
    "}\n",
    "formatted_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Method 2  \n",
    "Upon analysis, opportunities for enhancement in our cleaning method surfaced, leading to the implementation of a new cleaning function.  \n",
    "In the revised cleaning method (Method 2), emphasis was placed on stripping markdown text while adopting a strategy of replacing certain text elements to uphold the intended meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 5998 times.\n",
      "Returned original text 2 times.\n"
     ]
    }
   ],
   "source": [
    "# Function to convert Markdown to plain text\n",
    "def strip_markdown(text):\n",
    "    # Remove Markdown links\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]\\([^\\)]*\\)', r'\\1', text)\n",
    "    \n",
    "    # Remove Markdown emphasis (* or _)\n",
    "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)\n",
    "    \n",
    "    # Remove Markdown inline code (`)\n",
    "    text = re.sub(r'`([^`]+)`', r'\\1', text)\n",
    "    \n",
    "    # Remove Markdown headers (##, ###, etc.)\n",
    "    text = re.sub(r'#+\\s*(.*?)\\n', r'\\1\\n', text)\n",
    "    \n",
    "    # Remove other Markdown elements as needed\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Initialize counters for text cleaning\n",
    "cleaned_count = 0\n",
    "original_count = 0\n",
    "\n",
    "def clean_text(text):\n",
    "    global cleaned_count, original_count\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        original_count += 1\n",
    "        return text\n",
    "\n",
    "######################################\n",
    "#        Standardize The Text        #\n",
    "######################################\n",
    "\n",
    "    # Lowercasing should be one of the first steps to ensure uniformity\n",
    "    text = text.lower()\n",
    "\n",
    "######################################\n",
    "#         Remove Characters          #\n",
    "######################################\n",
    "\n",
    "    # Remove emojis, special characters, and punctuation\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n",
    "\n",
    "######################################\n",
    "#         Remove/Replace Text        #\n",
    "######################################\n",
    "\n",
    "    # Remove specific phrases \"Website or app\" and \"local react development\"\n",
    "    text = text.replace(\"website or app\", \"\")\n",
    "    text = text.replace(\"local react development\", \"\")\n",
    "\n",
    "    # Replace URLs, HTML tags, user mentions, and markdown image references\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub(r'<.*?>', '<HTML_TAG>', text)\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    text = re.sub(r'!\\[image\\]\\(.*?\\)', '<IMAGE>', text)\n",
    "\n",
    "    # Remove text starting with \"DevTools\" and ending with \"(automated)\"\n",
    "    text = re.sub(r'DevTools.*?\\(automated\\)', '', text)\n",
    "\n",
    "\n",
    "\n",
    "        # Strip markdown formatting\n",
    "    text = strip_markdown(text)\n",
    "\n",
    "######################################\n",
    "#        Tidy Up Whitespaces         #\n",
    "######################################\n",
    "\n",
    "    # Remove consecutive whitespaces and replace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "######################################\n",
    "#            Final Things            #\n",
    "######################################\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove words that are over 20 characters\n",
    "    words = [word for word in words if len(word) <= 20]\n",
    "\n",
    "    # Join the remaining words back into cleaned text\n",
    "    cleaned_text = ' '.join(words)\n",
    "\n",
    "    cleaned_count += 1\n",
    "    return cleaned_text\n",
    "\n",
    "# Applying clean_text function to test and train data\n",
    "test_data['body'] = test_data['body'].apply(clean_text)\n",
    "test_data['title'] = test_data['title'].apply(clean_text)\n",
    "\n",
    "train_data['body'] = train_data['body'].apply(clean_text)\n",
    "train_data['title'] = train_data['title'].apply(clean_text)\n",
    "\n",
    "# Displaying cleaning statistics\n",
    "print(f\"Cleaned {cleaned_count} times.\")\n",
    "print(f\"Returned original text {original_count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_facebook = test_data[: 300]\n",
    "test_data_tensorflow = test_data[300: 600]\n",
    "test_data_microsoft = test_data[600: 900]\n",
    "test_data_bitcoin = test_data[900: 1200]\n",
    "test_data_opencv= test_data[1200: 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_facebook = train_data[: 300]\n",
    "train_data_tensorflow = train_data[300: 600]\n",
    "train_data_microsoft = train_data[600: 900]\n",
    "train_data_bitcoin = train_data[900: 1200]\n",
    "train_data_opencv= train_data[1200: 1500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved models  \n",
    "Upon analyzing the step metrics of the fine-tuned models, it became evident that certain models, specifically those associated with the TensorFlow, Microsoft, and OpenCV repositories, exhibited training_loss figures indicating potential for improvement.\n",
    "\n",
    "Considering this insight, we opted to develop new fine-tuned models, augmenting the epochs and integrating the enhanced cleaning method for these specific repositories.\n",
    "\n",
    "## Tensorflow Improved model\n",
    "For this tensorflow improved model we utilized the cleaning method 2 and 10 epochs on the fine-tuning processes. All the initial models displayed above on this notebook used cleaning method 1 and 3 epochs on the fine-tunning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "max_content_tokens = 3999\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Function to truncate the message and avoid passing the limit of 4k tokens per gpt-3.5 fine-tuned model limitations\n",
    "def truncate_message(message, max_length):\n",
    "    tokens = encoding.encode(message)\n",
    "    if len(tokens) > max_length:\n",
    "        truncated_tokens = tokens[:max_length]\n",
    "        message = encoding.decode(truncated_tokens)\n",
    "    return message\n",
    "\n",
    "# Open the file in write mode\n",
    "with open('data/conversationaldata/conversational_data_tensorflow_new.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in train_data_tensorflow.iterrows():\n",
    "        # Create the user message by formatting the prompt with the title and body\n",
    "        user_message = f\"Classify, IN ONLY 1 WORD, the following GitHub issue as 'feature', 'bug', or 'question' based on its title and body:\\n{row['title']}\\n{row['body']}\"\n",
    "        \n",
    "        # Truncate the prompt if necessary\n",
    "        user_message = truncate_message(user_message, max_content_tokens)\n",
    "        # Create the assistant message by taking the label\n",
    "        assistant_message = row['label']\n",
    "        \n",
    "        # Construct the conversation object\n",
    "        conversation_object = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"GitHub Issue Report Classifier\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write the conversation object to one line in the file\n",
    "        f.write(json.dumps(conversation_object, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-BJCK5M6m756cgb0G6peUXb91', bytes=547089, created_at=1701744292, filename='conversational_data_tensorflow_new.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_file_tf_new = client.files.create(\n",
    "  file=open(\"data/conversationaldata/conversational_data_tensorflow_new.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft_file_tf_new.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-AbcuR8vh5M8kQDIV8hhLoXpe', created_at=1701744304, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=10, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-CpaRU3Zq9ePCCtbhezmcbgrg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-BJCK5M6m756cgb0G6peUXb91', validation_file=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a fine-tuned model\n",
    "ft_job_tf_new = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file_tf_new.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix= \"tf-issueclassifier\",\n",
    "  hyperparameters={\"n_epochs\": 10}\n",
    ")\n",
    "ft_job_tf_new.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:gcucst440:tf-issueclassifier:8SGhlOsl\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the state of a fine-tune\n",
    "tensorflow_ft_model_new = client.fine_tuning.jobs.retrieve(ft_job_tf_new.id).fine_tuned_model\n",
    "print(tensorflow_ft_model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can track the progress of your fine-tuning job by listing the lastest events. On our models it took about 3 hours to fine-tune each model\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_tf_new.id, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_tf_new = []\n",
    "y_pred_tf_new = []\n",
    "\n",
    "iterations = len(test_data_tensorflow)\n",
    "\n",
    "# Now let's loop through the test data and classify the GitHub pull requests\n",
    "for i in range(iterations):\n",
    "    correct_label = test_data_tensorflow.iloc[i]['label'].lower()\n",
    "    description = f\"{test_data_tensorflow.iloc[i]['title']} \\n {test_data_tensorflow.iloc[i]['body']}\"\n",
    "    print(f\"Correct PR type: {correct_label}\")\n",
    "    \n",
    "    prompt = f\"Classify, IN ONLY 1 WORD, the following GitHub pull request as 'feature', 'bug', or 'question' based on its title and body:\\n{description}\"\n",
    "    response = query_chatgpt(prompt, tensorflow_ft_model_new)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Failed to get a response after several retries. Skipping this item.\")\n",
    "        continue  # Skip this iteration and move to the next one\n",
    "    \n",
    "    # Clean the response to keep only letters (and optionally numbers)\n",
    "    predicted_label = re.sub(r'[^A-Za-z]+', '', response).lower().strip()\n",
    "    print(f\"Predicted PR type: {predicted_label}\")\n",
    "    \n",
    "    # Append to lists for evaluation\n",
    "    y_true_tf_new.append(correct_label)\n",
    "    y_pred_tf_new.append(predicted_label)\n",
    "    time.sleep(6)  # Wait for 6 seconds before retrying\n",
    "\n",
    "# See output on outputs/cell89output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: {'TP': 82, 'FP': 6, 'FN': 18, 'TN': 194}\n",
      "bug: {'TP': 88, 'FP': 9, 'FN': 12, 'TN': 191}\n",
      "question: {'TP': 91, 'FP': 24, 'FN': 9, 'TN': 176}\n",
      "Precision = 0.8767796748298766\n",
      "Recall = 0.87\n",
      "F1-score = 0.8707510228891061\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average F1-score, precision, and recall\n",
    "f1_tf_new = f1_score(y_true_tf_new, y_pred_tf_new, labels=labels, average='weighted')\n",
    "precision_tf_new = precision_score(y_true_tf_new, y_pred_tf_new, labels=labels, average='weighted')\n",
    "recall_tf_new = recall_score(y_true_tf_new, y_pred_tf_new, labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm_tf_new = confusion_matrix(y_true_tf_new, y_pred_tf_new, labels=labels)\n",
    "\n",
    "cm_df_tf_new = pd.DataFrame(cm_tf_new, index=labels, columns=labels)\n",
    "\n",
    "# Calculate TP, FP, FN, TN\n",
    "results_tf_new = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results_tf_new[label] = {'TP': cm_tf_new[i, i]}\n",
    "    results_tf_new[label]['FP'] = cm_tf_new[:, i].sum() - cm_tf_new[i, i]\n",
    "    results_tf_new[label]['FN'] = cm_tf_new[i, :].sum() - cm_tf_new[i, i]\n",
    "    results_tf_new[label]['TN'] = cm_tf_new.sum() - (results_tf_new[label]['TP'] + results_tf_new[label]['FP'] + results_tf_new[label]['FN'])\n",
    "\n",
    "# Print results_tf_new\n",
    "for label, metrics in results_tf_new.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results_tf_new to CSV\n",
    "results_tf_new_df = pd.DataFrame(results_tf_new).T\n",
    "results_tf_new_df['F1-score'] = f1_tf_new\n",
    "results_tf_new_df['Recall'] = recall_tf_new\n",
    "results_tf_new_df['Precision'] = precision_tf_new\n",
    "\n",
    "results_tf_new_df.to_csv('metrics/confusion_matrix_tf_new.csv')\n",
    "\n",
    "print(f\"Precision = {precision_tf_new}\")\n",
    "print(f\"Recall = {recall_tf_new}\")\n",
    "print(f\"F1-score = {f1_tf_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  recall  f1-score  support\n",
      "bug            0.907216    0.88  0.893401   100.00\n",
      "feature        0.931818    0.82  0.872340   100.00\n",
      "question       0.791304    0.91  0.846512   100.00\n",
      "accuracy       0.870000    0.87  0.870000     0.87\n",
      "macro avg      0.876780    0.87  0.870751   300.00\n",
      "weighted avg   0.876780    0.87  0.870751   300.00\n"
     ]
    }
   ],
   "source": [
    "# Create a classification report\n",
    "report_tf_new = classification_report(y_true_tf_new, y_pred_tf_new, labels=['bug', 'feature', 'question'], target_names=['bug', 'feature', 'question'], zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df_tf_new = pd.DataFrame(report_tf_new).transpose()\n",
    "\n",
    "# Print the classification report\n",
    "print(report_df_tf_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV Improved Model\n",
    "Cleaning method 2 and 6 epochs on fine-tuning process.\n",
    "\n",
    "Employing a new cleaning method alongside 6 epochs, we chose this iteration as our experimentation with 10 epochs indicated it to be excessive for our TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('data/conversationaldata/conversational_data_opencv_new.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in train_data_opencv.iterrows():\n",
    "        # Create the user message by formatting the prompt with the title and body\n",
    "        user_message = f\"Classify, IN ONLY 1 WORD, the following GitHub issue as 'feature', 'bug', or 'question' based on its title and body:\\n{row['title']}\\n{row['body']}\"\n",
    "        \n",
    "        # Truncate the prompt if necessary\n",
    "        user_message = truncate_message(user_message, max_content_tokens)\n",
    "        # Create the assistant message by taking the label\n",
    "        assistant_message = row['label']\n",
    "        \n",
    "        # Construct the conversation object\n",
    "        conversation_object = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"GitHub Issue Report Classifier\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write the conversation object to one line in the file\n",
    "        f.write(json.dumps(conversation_object, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-BXCWSF32dn7asPO4Nfxqyhq5', bytes=605037, created_at=1701752486, filename='conversational_data_opencv_new.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_file_oc_new = client.files.create(\n",
    "  file=open(\"data/conversationaldata/conversational_data_opencv_new.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "ft_file_oc_new.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-rNIHMHUIy97BY1eDQDttqEZy', created_at=1701752539, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=6, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-CpaRU3Zq9ePCCtbhezmcbgrg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-BXCWSF32dn7asPO4Nfxqyhq5', validation_file=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a fine-tuned model\n",
    "ft_job_oc_new = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file_oc_new.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix= \"oc-issueclassifier\",\n",
    "  hyperparameters={\"n_epochs\": 6}\n",
    ")\n",
    "ft_job_oc_new.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:gcucst440:oc-issueclassifier:8SJ2ph8V\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the state of a fine-tune\n",
    "opencv_ft_model_new = client.fine_tuning.jobs.retrieve(ft_job_oc_new.id).fine_tuned_model\n",
    "print(opencv_ft_model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can track the progress of your fine-tuning job by listing the lastest events. On our models it took about 3 hours to fine-tune each model\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_oc_new.id, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_oc_new = []\n",
    "y_pred_oc_new = []\n",
    "\n",
    "iterations = len(test_data_opencv)\n",
    "\n",
    "# Now let's loop through the test data and classify the GitHub pull requests\n",
    "for i in range(iterations):\n",
    "    correct_label = test_data_opencv.iloc[i]['label'].lower()\n",
    "    description = f\"{test_data_opencv.iloc[i]['title']}\\n{test_data_opencv.iloc[i]['body']}\"\n",
    "    print(f\"Correct PR type: {correct_label}\")\n",
    "    \n",
    "    prompt = f\"Classify, IN ONLY 1 WORD, the following GitHub pull request as 'feature', 'bug', or 'question' based on its title and body:\\n{description}\"\n",
    "    response = query_chatgpt(prompt, opencv_ft_model_new)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Failed to get a response after several retries. Skipping this item.\")\n",
    "        continue  # Skip this iteration and move to the next one\n",
    "    \n",
    "    # Clean the response to keep only letters (and optionally numbers)\n",
    "    predicted_label = re.sub(r'[^A-Za-z]+', '', response).lower().strip()\n",
    "    print(f\"Predicted PR type: {predicted_label}\")\n",
    "    \n",
    "    # Append to lists for evaluation\n",
    "    y_true_oc_new.append(correct_label)\n",
    "    y_pred_oc_new.append(predicted_label)\n",
    "    time.sleep(6)  # Wait for 6 seconds before retrying\n",
    "\n",
    "# See output on outputs/cell98output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: {'TP': 80, 'FP': 8, 'FN': 20, 'TN': 192}\n",
      "bug: {'TP': 86, 'FP': 32, 'FN': 14, 'TN': 168}\n",
      "question: {'TP': 81, 'FP': 13, 'FN': 19, 'TN': 187}\n",
      "Precision = 0.8332021986908391\n",
      "Recall = 0.8233333333333334\n",
      "F1-score = 0.8250354006223534\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average F1-score, precision, and recall\n",
    "f1_oc_new = f1_score(y_true_oc_new, y_pred_oc_new, labels=labels, average='weighted')\n",
    "precision_oc_new = precision_score(y_true_oc_new, y_pred_oc_new, labels=labels, average='weighted')\n",
    "recall_oc_new = recall_score(y_true_oc_new, y_pred_oc_new, labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm_oc_new = confusion_matrix(y_true_oc_new, y_pred_oc_new, labels=labels)\n",
    "\n",
    "cm_df_oc_new = pd.DataFrame(cm_oc_new, index=labels, columns=labels)\n",
    "\n",
    "# Calculate TP, FP, FN, TN\n",
    "results_oc_new = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results_oc_new[label] = {'TP': cm_oc_new[i, i]}\n",
    "    results_oc_new[label]['FP'] = cm_oc_new[:, i].sum() - cm_oc_new[i, i]\n",
    "    results_oc_new[label]['FN'] = cm_oc_new[i, :].sum() - cm_oc_new[i, i]\n",
    "    results_oc_new[label]['TN'] = cm_oc_new.sum() - (results_oc_new[label]['TP'] + results_oc_new[label]['FP'] + results_oc_new[label]['FN'])\n",
    "\n",
    "# Print results_oc_new\n",
    "for label, metrics in results_oc_new.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results_oc_new to CSV\n",
    "results_oc_new_df = pd.DataFrame(results_oc_new).T\n",
    "results_oc_new_df['F1-score'] = f1_oc_new\n",
    "results_oc_new_df['Recall'] = recall_oc_new\n",
    "results_oc_new_df['Precision'] = precision_oc_new\n",
    "\n",
    "results_oc_new_df.to_csv('metrics/confusion_matrix_oc_new.csv')\n",
    "\n",
    "print(f\"Precision = {precision_oc_new}\")\n",
    "print(f\"Recall = {recall_oc_new}\")\n",
    "print(f\"F1-score = {f1_oc_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score     support\n",
      "bug            0.728814  0.860000  0.788991  100.000000\n",
      "feature        0.909091  0.800000  0.851064  100.000000\n",
      "question       0.861702  0.810000  0.835052  100.000000\n",
      "accuracy       0.823333  0.823333  0.823333    0.823333\n",
      "macro avg      0.833202  0.823333  0.825035  300.000000\n",
      "weighted avg   0.833202  0.823333  0.825035  300.000000\n"
     ]
    }
   ],
   "source": [
    "# Create a classification report\n",
    "report_oc_new = classification_report(y_true_oc_new, y_pred_oc_new, labels=['bug', 'feature', 'question'], target_names=['bug', 'feature', 'question'], zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df_oc_new = pd.DataFrame(report_oc_new).transpose()\n",
    "\n",
    "# Print the classification report\n",
    "print(report_df_oc_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft Improved model\n",
    "Cleaning Method 1 and 6 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-ZRVLAfBawJPXM5OB06FPLSur', created_at=1701829890, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size='auto', learning_rate_multiplier='auto'), model='ft:gpt-3.5-turbo-0613:gcucst440:ms-issueclassifier:8LLFl5QI', object='fine_tuning.job', organization_id='org-CpaRU3Zq9ePCCtbhezmcbgrg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-o2pr8FckpXuZhncbVF9Fykzl', validation_file=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a fine-tuned model\n",
    "ft_job_ms_new = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file_ms, ## Using same file with old cleaning method\n",
    "  model=microsoft_ft_model, ## Using old model as the base model\n",
    "  suffix= \"ms-issueclassifier\",\n",
    "  hyperparameters={\"n_epochs\": 3}\n",
    ")\n",
    "ft_job_ms_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:gcucst440:ms-issueclassifier:8ScXRY4K\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the state of a fine-tune\n",
    "microsoft_ft_model_new = client.fine_tuning.jobs.retrieve(ft_job_ms_new.id).fine_tuned_model\n",
    "print(microsoft_ft_model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can track the progress of your fine-tuning job by listing the lastest events. On our models it took about 3 hours to fine-tune each model\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_job_ms_new.id, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_ms_new = []\n",
    "y_pred_ms_new = []\n",
    "\n",
    "iterations = len(test_data_microsoft)\n",
    "\n",
    "# Now let's loop through the test data and classify the GitHub pull requests\n",
    "for i in range(iterations):\n",
    "    correct_label = test_data_microsoft.iloc[i]['label'].lower()\n",
    "    description = f\"{test_data_microsoft.iloc[i]['title']}\\n{test_data_microsoft.iloc[i]['body']}\"\n",
    "    print(f\"Correct PR type: {correct_label}\")\n",
    "    \n",
    "    prompt = f\"Classify, IN ONLY 1 WORD, the following GitHub pull request as 'feature', 'bug', or 'question' based on its title and body:\\n{description}\"\n",
    "    response = query_chatgpt(prompt, microsoft_ft_model_new)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Failed to get a response after several retries. Skipping this item.\")\n",
    "        continue  # Skip this iteration and move to the next one\n",
    "    \n",
    "    # Clean the response to keep only letters (and optionally numbers)\n",
    "    predicted_label = re.sub(r'[^A-Za-z]+', '', response).lower().strip()\n",
    "    print(f\"Predicted PR type: {predicted_label}\")\n",
    "    \n",
    "    # Append to lists for evaluation\n",
    "    y_true_ms_new.append(correct_label)\n",
    "    y_pred_ms_new.append(predicted_label)\n",
    "    time.sleep(6)  # Wait for 6 seconds before retrying\n",
    "\n",
    "# See output on outputs/cell105output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: {'TP': 87, 'FP': 20, 'FN': 13, 'TN': 180}\n",
      "bug: {'TP': 80, 'FP': 14, 'FN': 20, 'TN': 186}\n",
      "question: {'TP': 79, 'FP': 20, 'FN': 21, 'TN': 180}\n",
      "Precision = 0.8207092466388549\n",
      "Recall = 0.82\n",
      "F1-score = 0.8197639424774652\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average F1-score, precision, and recall\n",
    "f1_ms_new = f1_score(y_true_ms_new, y_pred_ms_new, labels=labels, average='weighted')\n",
    "precision_ms_new = precision_score(y_true_ms_new, y_pred_ms_new, labels=labels, average='weighted')\n",
    "recall_ms_new = recall_score(y_true_ms_new, y_pred_ms_new, labels=labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm_ms_new = confusion_matrix(y_true_ms_new, y_pred_ms_new, labels=labels)\n",
    "\n",
    "cm_df_ms_new = pd.DataFrame(cm_ms_new, index=labels, columns=labels)\n",
    "\n",
    "# Calculate TP, FP, FN, TN\n",
    "results_ms_new = {}\n",
    "for i, label in enumerate(labels):\n",
    "    results_ms_new[label] = {'TP': cm_ms_new[i, i]}\n",
    "    results_ms_new[label]['FP'] = cm_ms_new[:, i].sum() - cm_ms_new[i, i]\n",
    "    results_ms_new[label]['FN'] = cm_ms_new[i, :].sum() - cm_ms_new[i, i]\n",
    "    results_ms_new[label]['TN'] = cm_ms_new.sum() - (results_ms_new[label]['TP'] + results_ms_new[label]['FP'] + results_ms_new[label]['FN'])\n",
    "\n",
    "# Print results_ms_new\n",
    "for label, metrics in results_ms_new.items():\n",
    "    print(f\"{label}: {metrics}\")\n",
    "\n",
    "# Save results_ms_new to CSV\n",
    "results_ms_new_df = pd.DataFrame(results_ms_new).T\n",
    "results_ms_new_df['F1-score'] = f1_ms_new\n",
    "results_ms_new_df['Recall'] = recall_ms_new\n",
    "results_ms_new_df['Precision'] = precision_ms_new\n",
    "\n",
    "\n",
    "results_ms_new_df.to_csv('metrics/confusion_matrix_ms_new.csv')\n",
    "\n",
    "print(f\"Precision = {precision_ms_new}\")\n",
    "print(f\"Recall = {recall_ms_new}\")\n",
    "print(f\"F1-score = {f1_ms_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  recall  f1-score  support\n",
      "bug            0.851064    0.80  0.824742   100.00\n",
      "feature        0.813084    0.87  0.840580   100.00\n",
      "question       0.797980    0.79  0.793970   100.00\n",
      "accuracy       0.820000    0.82  0.820000     0.82\n",
      "macro avg      0.820709    0.82  0.819764   300.00\n",
      "weighted avg   0.820709    0.82  0.819764   300.00\n"
     ]
    }
   ],
   "source": [
    "# Create a classification report\n",
    "report_ms_new = classification_report(y_true_ms_new, y_pred_ms_new, labels=['bug', 'feature', 'question'], target_names=['bug', 'feature', 'question'], zero_division=0, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df_ms_new = pd.DataFrame(report_ms_new).transpose()\n",
    "\n",
    "# Print the classification report\n",
    "print(report_df_ms_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Results\n",
    "\n",
    "To obtain the overall results we utilized the metrics from the regular facebook and bitcoin models, and the improved tensorflow, microsoft and opencv models. We then calculated the average of each metrics to have a complete overall metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Results: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Bug': {'Precision': 0.8108599999999999, 'Recall': 0.858, 'F1-Score': 0.8321},\n",
       " 'Feature': {'Precision': 0.8683,\n",
       "  'Recall': 0.8540000000000001,\n",
       "  'F1-Score': 0.85928},\n",
       " 'Question': {'Precision': 0.8183,\n",
       "  'Recall': 0.774,\n",
       "  'F1-Score': 0.7925000000000001},\n",
       " 'Average': {'Precision': 0.8324199999999999,\n",
       "  'Recall': 0.82866,\n",
       "  'F1-Score': 0.828}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_average(values):\n",
    "    total = 0\n",
    "    for value in values:\n",
    "        total += value\n",
    "    return total/len(values)\n",
    "\n",
    "### BUG ###\n",
    "facebook_bug_precision = 0.8333 ## Obtained in cell 56\n",
    "tensorflow_bug_precision = 0.9072 ## Obtained in cell 91\n",
    "microsoft_bug_precision = 0.8511 ## Obtained in cell 107\n",
    "bitcoin_bug_precision = 0.7339 ## Obtained in cell 71\n",
    "opencv_bug_precision = 0.7288 ## Obtained in cell 100\n",
    "bug_precision_values = [facebook_bug_precision, tensorflow_bug_precision, microsoft_bug_precision, bitcoin_bug_precision, opencv_bug_precision]\n",
    "\n",
    "facebook_bug_recall = 0.9500 ## Obtained in cell 56\n",
    "tensorflow_bug_recall = 0.8800 ## Obtained in cell 91\n",
    "microsoft_bug_recall = 0.8000 ## Obtained in cell 107\n",
    "bitcoin_bug_recall = 0.8000 ## Obtained in cell 71\n",
    "opencv_bug_recall = 0.8600 ## Obtained in cell 100\n",
    "bug_recall_values = [facebook_bug_recall, tensorflow_bug_recall, microsoft_bug_recall, bitcoin_bug_recall, opencv_bug_recall]\n",
    "\n",
    "facebook_bug_f1 = 0.8878 ## Obtained in cell 56\n",
    "tensorflow_bug_f1 = 0.8934 ## Obtained in cell 91\n",
    "microsoft_bug_f1 = 0.8247 ## Obtained in cell 107\n",
    "bitcoin_bug_f1 = 0.7656 ## Obtained in cell 71\n",
    "opencv_bug_f1 = 0.7890 ## Obtained in cell 100\n",
    "bug_f1_values = [facebook_bug_f1, tensorflow_bug_f1, microsoft_bug_f1, bitcoin_bug_f1, opencv_bug_f1]\n",
    "\n",
    "\n",
    "### FEATURE ###\n",
    "facebook_feature_precision = 0.8557 ## Obtained in cell 56\n",
    "tensorflow_feature_precision = 0.9318 ## Obtained in cell 91\n",
    "microsoft_feature_precision = 0.8131## Obtained in cell 107\n",
    "bitcoin_feature_precision = 0.8318 ## Obtained in cell 71\n",
    "opencv_feature_precision = 0.9091 ## Obtained in cell 100\n",
    "feature_precision_values = [facebook_feature_precision, tensorflow_feature_precision, microsoft_feature_precision, bitcoin_feature_precision, opencv_feature_precision]\n",
    "\n",
    "facebook_feature_recall = 0.8900 ## Obtained in cell 56\n",
    "tensorflow_feature_recall = 0.8200 ## Obtained in cell 91\n",
    "microsoft_feature_recall = 0.8700 ## Obtained in cell 107\n",
    "bitcoin_feature_recall = 0.8900 ## Obtained in cell 71\n",
    "opencv_feature_recall = 0.8000 ## Obtained in cell 100\n",
    "feature_recall_values = [facebook_feature_recall, tensorflow_feature_recall, microsoft_feature_recall, bitcoin_feature_recall, opencv_feature_recall]\n",
    "\n",
    "facebook_feature_f1 = 0.8725 ## Obtained in cell 56\n",
    "tensorflow_feature_f1 = 0.8723 ## Obtained in cell 91\n",
    "microsoft_feature_f1 = 0.8406 ## Obtained in cell 107\n",
    "bitcoin_feature_f1 = 0.8599 ## Obtained in cell 71\n",
    "opencv_feature_f1 = 0.8511 ## Obtained in cell 100\n",
    "feature_f1_values = [facebook_feature_f1, tensorflow_feature_f1, microsoft_feature_f1, bitcoin_feature_f1, opencv_feature_f1]\n",
    "\n",
    "\n",
    "### QUESTION ###\n",
    "facebook_question_precision = 0.9024 ## Obtained in cell 56\n",
    "tensorflow_question_precision = 0.7913 ## Obtained in cell 91\n",
    "microsoft_question_precision = 0.7980 ## Obtained in cell 107\n",
    "bitcoin_question_precision = 0.7381 ## Obtained in cell 71\n",
    "opencv_question_precision = 0.8617 ## Obtained in cell 100\n",
    "question_precision_values = [facebook_question_precision, tensorflow_question_precision, microsoft_question_precision, bitcoin_question_precision, opencv_question_precision]\n",
    "\n",
    "facebook_question_recall = 0.7400 ## Obtained in cell 56\n",
    "tensorflow_question_recall = 0.9100 ## Obtained in cell 91\n",
    "microsoft_question_recall = 0.7900 ## Obtained in cell 107\n",
    "bitcoin_question_recall = 0.6200 ## Obtained in cell 71\n",
    "opencv_question_recall = 0.8100 ## Obtained in cell 100\n",
    "question_recall_values = [facebook_question_recall, tensorflow_question_recall, microsoft_question_recall, bitcoin_question_recall, opencv_question_recall]\n",
    "\n",
    "facebook_question_f1 = 0.8132 ## Obtained in cell 56\n",
    "tensorflow_question_f1 = 0.8465 ## Obtained in cell 91\n",
    "microsoft_question_f1 = 0.7938 ## Obtained in cell 107\n",
    "bitcoin_question_f1 = 0.6739 ## Obtained in cell 71\n",
    "opencv_question_f1 = 0.8351 ## Obtained in cell 100\n",
    "question_f1_values = [facebook_question_f1, tensorflow_question_f1, microsoft_question_f1, bitcoin_question_f1, opencv_question_f1]\n",
    "\n",
    "\n",
    "### AVERAGE ###\n",
    "facebook_average_precision = 0.8635 ## Obtained in cell 56\n",
    "tensorflow_average_precision = 0.8768 ## Obtained in cell 91\n",
    "microsoft_average_precision = 0.8207 ## Obtained in cell 107\n",
    "bitcoin_average_precision = 0.7679 ## Obtained in cell 71 \n",
    "opencv_average_precision = 0.8332 ## Obtained in cell 100\n",
    "average_precision_values = [facebook_average_precision, tensorflow_average_precision, microsoft_average_precision, bitcoin_average_precision, opencv_average_precision]\n",
    "\n",
    "facebook_average_recall = 0.8600 ## Obtained in cell 56\n",
    "tensorflow_average_recall = 0.8700 ## Obtained in cell 91\n",
    "microsoft_average_recall = 0.8200 ## Obtained in cell 107\n",
    "bitcoin_average_recall = 0.7700 ## Obtained in cell 71\n",
    "opencv_average_recall = 0.8233 ## Obtained in cell 100\n",
    "average_recall_values = [facebook_average_recall, tensorflow_average_recall, microsoft_average_recall, bitcoin_average_recall, opencv_average_recall]\n",
    "\n",
    "facebook_average_f1 = 0.8579 ## Obtained in cell 56\n",
    "tensorflow_average_f1 = 0.8708 ## Obtained in cell 91\n",
    "microsoft_average_f1 = 0.8198 ## Obtained in cell 107\n",
    "bitcoin_average_f1 = 0.7665 ## Obtained in cell 71\n",
    "opencv_average_f1 = 0.8250 ## Obtained in cell 100\n",
    "average_f1_values = [facebook_average_f1, tensorflow_average_f1, microsoft_average_f1, bitcoin_average_f1, opencv_average_f1]\n",
    "\n",
    "# Calculating overall (average) values  \n",
    "overall_bug_f1 = calculate_average(bug_f1_values)\n",
    "overall_bug_precision = calculate_average(bug_precision_values)\n",
    "overall_bug_recall = calculate_average(bug_recall_values)\n",
    "overall_feature_f1 = calculate_average(feature_f1_values)\n",
    "overall_feature_precision = calculate_average(feature_precision_values)\n",
    "overall_feature_recall = calculate_average(feature_recall_values)\n",
    "overall_question_f1 = calculate_average(question_f1_values)\n",
    "overall_question_precision = calculate_average(question_precision_values)\n",
    "overall_question_recall = calculate_average(question_recall_values)\n",
    "overall_average_f1 = calculate_average(average_f1_values)\n",
    "overall_average_precision = calculate_average(average_precision_values)\n",
    "overall_average_recall = calculate_average(average_recall_values)\n",
    "\n",
    "print(\"Overall Results: \")\n",
    "# Formatting the results\n",
    "formatted_metrics = {\n",
    "    \"Bug\": {\n",
    "        \"Precision\": overall_bug_precision, \n",
    "        \"Recall\": overall_bug_recall, \n",
    "        \"F1-Score\": overall_bug_f1\n",
    "    },\n",
    "    \"Feature\": {\n",
    "        \"Precision\": overall_feature_precision, \n",
    "        \"Recall\": overall_feature_recall, \n",
    "        \"F1-Score\": overall_feature_f1\n",
    "    },\n",
    "    \"Question\": {\n",
    "        \"Precision\": overall_question_precision, \n",
    "        \"Recall\": overall_question_recall, \n",
    "        \"F1-Score\": overall_question_f1\n",
    "    },\n",
    "    \"Average\": {\n",
    "        \"Precision\": overall_average_precision, \n",
    "        \"Recall\": overall_average_recall, \n",
    "        \"F1-Score\": overall_average_f1\n",
    "    }\n",
    "}\n",
    "formatted_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
